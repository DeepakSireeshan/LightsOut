{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92f297a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 141\u001b[0m\n\u001b[0;32m    139\u001b[0m temp_grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mtuple\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m grid_value])\n\u001b[0;32m    140\u001b[0m next_act \u001b[38;5;241m=\u001b[39m next_action(temp_grid, epsilon) \u001b[38;5;66;03m#next action to take based off of epsilon greedy algorithm\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m action_click(next_act)\n\u001b[0;32m    142\u001b[0m reward \u001b[38;5;241m=\u001b[39m reward_for_state(grid_value)\n\u001b[0;32m    143\u001b[0m old_q_value \u001b[38;5;241m=\u001b[39m q_values[(temp_grid, next_act)]\n",
      "Cell \u001b[1;32mIn[46], line 102\u001b[0m, in \u001b[0;36maction_click\u001b[1;34m(action)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maction_click\u001b[39m(action):\n\u001b[1;32m--> 102\u001b[0m     row \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    103\u001b[0m     col \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    104\u001b[0m     grid_value[row][col] \u001b[38;5;241m^\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import time \n",
    "import numpy as np\n",
    "import copy\n",
    "from random import *\n",
    "from matinv import *\n",
    "\n",
    "# Q leaning:\n",
    "# Environment: ALl possible grid positions, a total of 2^9 = 512\n",
    "\n",
    "\n",
    "#Actions: clicking one of the 9 grids numbered 0 to 8\n",
    "# Rewards:\n",
    "\n",
    "#create a set of all states that are one move away from completing(total of 9 states):\n",
    "# Each action can be represented as a tuple (i,j) which indicated i-th row, j-th column grid is the button to be clicked.\n",
    "def states_with_actions(n):\n",
    "    states_dict = {}\n",
    "    for i in range(2**n):\n",
    "        binary_array = []\n",
    "        for j in range(n):\n",
    "            binary_array.append((i >> j) & 1)\n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                states_dict[(tuple([tuple(binary_array[i:i+3]) for i in range(0,9,3)]), (x,y))] = 0\n",
    "    return states_dict\n",
    "\n",
    "#Reward function:\n",
    "#--------------------\n",
    "#After every move, this function compares the current grid state and the initial grid state to compare how many \n",
    "#lights they have in common using bitwise XOR. The negative of sum of this array will be the reward for that state. \n",
    "#Note that when the initial grid and the current grid state are the same, the reward is zero (Maximum reward). \n",
    "#-----------------------\n",
    "\n",
    "def special_states():\n",
    "    arrays = []\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            empty_grid = [[0,0,0] for _ in range(3)]\n",
    "            empty_grid[row][col] ^= 1\n",
    "            if row - 1 >= 0:\n",
    "                empty_grid[row - 1][col] ^= 1  # Cell above\n",
    "            if row + 1 <= 2:\n",
    "                empty_grid[row + 1][col] ^= 1  # Cell below\n",
    "            if col - 1 >= 0:\n",
    "                empty_grid[row][col - 1] ^= 1  # Cell to the left\n",
    "            if col + 1 <= 2:\n",
    "                empty_grid[row][col + 1] ^= 1  # Cell to the right\n",
    "            arrays.append(empty_grid)\n",
    "    return arrays\n",
    "\n",
    "spl_grids = special_states()\n",
    "\n",
    "def reward_for_state(list_of_lists):\n",
    "    total = sum([sum(item) for item in list_of_lists])\n",
    "    if total == 0:\n",
    "        return 100\n",
    "    elif list_of_lists in spl_grids:\n",
    "        return -1\n",
    "    elif (total > 3):\n",
    "        return (-1)*total\n",
    "    else:\n",
    "        return -5\n",
    "\n",
    "\n",
    "# Helper functions for the model\n",
    "\n",
    "#Function to find whether a given state is terminal\n",
    "def is_terminal(list_of_lists):\n",
    "    if reward_for_state(list_of_lists) == 100:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "#Function to find the max q-value of a state over all actions\n",
    "\n",
    "def best_action(tuple1, my_dict):\n",
    "    max_value = float('-inf')  # Initialize with negative infinity\n",
    "    max_key = None\n",
    "\n",
    "    for key, value in my_dict.items():\n",
    "        if key[0] == tuple1 and value > max_value:\n",
    "            max_value = value\n",
    "            max_key = key[1]\n",
    "\n",
    "    return max_key, max_value\n",
    "\n",
    "def start_action():\n",
    "    return (np.random.randint(3),np.random.randint(3))\n",
    "\n",
    "#Epsilon greedy algorithm for choosing the next action\n",
    "def next_action(grid, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        key, value = best_action(grid, q_values)\n",
    "        return key\n",
    "    else:\n",
    "        return (np.random.randint(3),np.random.randint(3))\n",
    "\n",
    "# Function imitating the action of clicking a grid to toggle the states of itself and its neighbors\n",
    "def action_click(action):\n",
    "    row = action[0]\n",
    "    col = action[1]\n",
    "    grid_value[row][col] ^= 1\n",
    "    if row - 1 >= 0:\n",
    "        grid_value[row - 1][col] ^= 1  # Cell above\n",
    "    if row + 1 <= 2:\n",
    "        grid_value[row + 1][col] ^= 1  # Cell below\n",
    "    if col - 1 >= 0:\n",
    "        grid_value[row][col - 1] ^= 1  # Cell to the left\n",
    "    if col + 1 <= 2:\n",
    "        grid_value[row][col + 1] ^= 1  # Cell to the right\n",
    "\n",
    "def solution(grid_value):\n",
    "    if is_terminal(grid_value):\n",
    "        return []\n",
    "    else:\n",
    "        state = (np.random.randint(3),np.random.randint(3))\n",
    "        solution = []\n",
    "        solution.append(state)\n",
    "        action_click(state)\n",
    "        while not is_terminal(grid_value):\n",
    "            grid = tuple([tuple(i) for i in grid_value])\n",
    "            next_act, val = best_action(grid, q_values)\n",
    "            action_click(next_act)\n",
    "            if next_act in solution:\n",
    "                solution.remove(next_act)\n",
    "            else:\n",
    "                solution.append(next_act)\n",
    "        return solution\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    q_values = states_with_actions(9):\n",
    "\n",
    "    epsilon = 0.9\n",
    "    discount = 0.9\n",
    "    learning_rate = 0.9\n",
    "\n",
    "\n",
    "    start = time.time()               \n",
    "    for episode in range(10000):\n",
    "        initial_grid = []\n",
    "        for _ in range(3):\n",
    "            initial_grid.append([randint(0,1) for b in range(0,3)])\n",
    "        grid_value = copy.deepcopy(initial_grid)\n",
    "        #print(grid_value)\n",
    "        state = start_action()\n",
    "        action_click(state)\n",
    "        while not is_terminal(grid_value):\n",
    "            #print(grid_value)\n",
    "            temp_grid = tuple([tuple(i) for i in grid_value])\n",
    "            next_act = next_action(temp_grid, epsilon) #next action to take based off of epsilon greedy algorithm\n",
    "            action_click(next_act)\n",
    "            reward = reward_for_state(grid_value)\n",
    "            old_q_value = q_values[(temp_grid, next_act)]\n",
    "            tuple1 = tuple([tuple(i) for i in grid_value])\n",
    "            key, max_q_value = best_action(tuple1, q_values) \n",
    "            temporal_diff = reward + (discount * max_q_value) - old_q_value\n",
    "            new_q_value = old_q_value + (learning_rate * temporal_diff)\n",
    "            q_values[(temp_grid, next_act)] = new_q_value\n",
    "\n",
    "    print('Done')\n",
    "    end = time.time()\n",
    "    print(end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ffcebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_grid = [[1,1,1],[1,1,1],[1,1,1]]\n",
    "grid_value = copy.deepcopy(initial_grid)\n",
    "\n",
    "with open('all_states_LO3.pkl', 'rb') as f:\n",
    "    q_values = pickle.load(f)\n",
    "\n",
    "def solution(grid_value):\n",
    "    if is_terminal(grid_value):\n",
    "        return []\n",
    "    else:\n",
    "        state = (np.random.randint(3),np.random.randint(3))\n",
    "        solution = []\n",
    "        solution.append(state)\n",
    "        action_click(state)\n",
    "        while not is_terminal(grid_value):\n",
    "            grid = tuple([tuple(i) for i in grid_value])\n",
    "            next_act, val = best_action(grid, q_values)\n",
    "            action_click(next_act)\n",
    "            if next_act in solution:\n",
    "                solution.remove(next_act)\n",
    "            else:\n",
    "                solution.append(next_act)\n",
    "        return solution\n",
    "\n",
    "x = solution(grid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82d48803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0g0\n",
      "0g2\n",
      "2g2\n",
      "2g0\n",
      "1g1\n"
     ]
    }
   ],
   "source": [
    "for (row,col) in x:\n",
    "    print(str(row)+'g'+str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec39f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_value = [[1,1,1] for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88cce618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 0), (2, 2), (0, 0), (0, 2)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution(grid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670a115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
