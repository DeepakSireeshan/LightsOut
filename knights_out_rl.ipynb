{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92f297a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "50.173983573913574\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import time \n",
    "import numpy as np\n",
    "import copy\n",
    "from random import *\n",
    "from matinv import *\n",
    "\n",
    "# Q leaning:\n",
    "# Environment: ALl possible grid positions, a total of 2^9 = 512\n",
    "\n",
    "\n",
    "#Actions: clicking one of the 9 grids numbered 0 to 8\n",
    "# Rewards:\n",
    "\n",
    "#create a set of all states that are one move away from completing(total of 9 states):\n",
    "# Each action can be represented as a tuple (i,j) which indicated i-th row, j-th column grid is the button to be clicked.\n",
    "def states_with_actions(n):\n",
    "    states_dict = {}\n",
    "    for i in range(2**n):\n",
    "        binary_array = []\n",
    "        for j in range(n):\n",
    "            binary_array.append((i >> j) & 1)\n",
    "        for x in range(5):\n",
    "            for y in range(5):\n",
    "                states_dict[(tuple([tuple(binary_array[i:i+5]) for i in range(0,25,5)]), (x,y))] = 0\n",
    "    return states_dict\n",
    "\n",
    "#Reward function:\n",
    "#--------------------\n",
    "#After every move, this function compares the current grid state and the initial grid state to compare how many \n",
    "#lights they have in common using bitwise XOR. The negative of sum of this array will be the reward for that state. \n",
    "#Note that when the initial grid and the current grid state are the same, the reward is zero (Maximum reward). \n",
    "#-----------------------\n",
    "\n",
    "def special_states():\n",
    "    arrays = []\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            empty_grid = [[0,0,0] for _ in range(3)]\n",
    "            possible_moves = [\n",
    "                (row - 2, col - 1),\n",
    "                (row - 2, col + 1),\n",
    "                (row - 1, col - 2),\n",
    "                (row - 1, col + 2),\n",
    "                (row + 1, col - 2),\n",
    "                (row + 1, col + 2),\n",
    "                (row + 2, col - 1),\n",
    "                (row + 2, col + 1)\n",
    "            ]\n",
    "            empty_grid[row][col] ^= 1\n",
    "            for move_row, move_col in possible_moves:\n",
    "                if 0 <= move_row < 5 and 0 <= move_col < 5:\n",
    "                    empty_grid [move_row][move_col] ^= 1\n",
    "            arrays.append(empty_grid)\n",
    "    return arrays\n",
    "\n",
    "spl_grids = special_states()\n",
    "\n",
    "def reward_for_state(list_of_lists):\n",
    "    total = sum([sum(item) for item in list_of_lists])\n",
    "    if total == 0:\n",
    "        return 100\n",
    "    elif list_of_lists in spl_grids:\n",
    "        return -1\n",
    "    elif (total > 3):\n",
    "        return (-1)*total\n",
    "    else:\n",
    "        return -5\n",
    "\n",
    "\n",
    "# Helper functions for the model\n",
    "\n",
    "#Function to find whether a given state is terminal\n",
    "def is_terminal(list_of_lists):\n",
    "    if reward_for_state(list_of_lists) == 100:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "#Function to find the max q-value of a state over all actions\n",
    "\n",
    "def best_action(tuple1, my_dict):\n",
    "    max_value = float('-inf')  # Initialize with negative infinity\n",
    "    max_key = None\n",
    "\n",
    "    for key, value in my_dict.items():\n",
    "        if key[0] == tuple1 and value > max_value:\n",
    "            max_value = value\n",
    "            max_key = key[1]\n",
    "\n",
    "    return max_key, max_value\n",
    "\n",
    "def start_action():\n",
    "    return (np.random.randint(3),np.random.randint(3))\n",
    "\n",
    "#Epsilon greedy algorithm for choosing the next action\n",
    "def next_action(grid, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        key, value = best_action(grid, q_values)\n",
    "        return key\n",
    "    else:\n",
    "        return (np.random.randint(3),np.random.randint(3))\n",
    "\n",
    "# Function imitating the action of clicking a grid to toggle the states of itself and its neighbors\n",
    "def action_click(action):\n",
    "    row = action[0]\n",
    "    col = action[1]\n",
    "    grid_value[row][col] ^= 1\n",
    "    if row - 1 >= 0:\n",
    "        grid_value[row - 1][col] ^= 1  # Cell above\n",
    "    if row + 1 <= 2:\n",
    "        grid_value[row + 1][col] ^= 1  # Cell below\n",
    "    if col - 1 >= 0:\n",
    "        grid_value[row][col - 1] ^= 1  # Cell to the left\n",
    "    if col + 1 <= 2:\n",
    "        grid_value[row][col + 1] ^= 1  # Cell to the right\n",
    "\n",
    "    \n",
    "#Function to get the solution\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "q_values = states_with_actions(9)\n",
    "\n",
    "epsilon = 0.9\n",
    "discount = 0.9\n",
    "learning_rate = 0.9\n",
    "\n",
    "\n",
    "start = time.time()               \n",
    "for episode in range(10000):\n",
    "    initial_grid = []\n",
    "    for _ in range(3):\n",
    "        initial_grid.append([randint(0,1) for b in range(0,3)])\n",
    "    grid_value = copy.deepcopy(initial_grid)\n",
    "    #print(grid_value)\n",
    "    state = start_action()\n",
    "    action_click(state)\n",
    "    while not is_terminal(grid_value):\n",
    "        #print(grid_value)\n",
    "        temp_grid = tuple([tuple(i) for i in grid_value])\n",
    "        next_act = next_action(temp_grid, epsilon) #next action to take based off of epsilon greedy algorithm\n",
    "        action_click(next_act)\n",
    "        reward = reward_for_state(grid_value)\n",
    "        old_q_value = q_values[(temp_grid, next_act)]\n",
    "        tuple1 = tuple([tuple(i) for i in grid_value])\n",
    "        key, max_q_value = best_action(tuple1, q_values) \n",
    "        temporal_diff = reward + (discount * max_q_value) - old_q_value\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_diff)\n",
    "        q_values[(temp_grid, next_act)] = new_q_value\n",
    "    \n",
    "print('Done')\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "# def on_grid_click(row, col):\n",
    "#     grid_value[row][col] ^= 1\n",
    "#     if row - 1 >= 0:\n",
    "#         grid_value[row - 1][col] ^= 1  # Cell above\n",
    "#     if row + 1 <= 2:\n",
    "#         grid_value[row + 1][col] ^= 1  # Cell below\n",
    "#     if col - 1 >= 0:\n",
    "#         grid_value[row][col - 1] ^= 1  # Cell to the left\n",
    "#     if col + 1 <= 2:\n",
    "#         grid_value[row][col + 1] ^= 1  # Cell to the right \n",
    "#     update_grid()\n",
    "    \n",
    "#     max_moves_counter.config(text=f\"Moves Needed: {sum(solver_3(grid_value))}\")\n",
    "    \n",
    "# def on_enter(event, row, col):\n",
    "#     canvas.itemconfig(grid_cells[row][col], fill='yellow')\n",
    "\n",
    "# def on_leave(event, row, col):\n",
    "#     cell_value = grid_value[row][col]\n",
    "#     if cell_value == 1:\n",
    "#         cell_color = 'blue'\n",
    "#     else:\n",
    "#         cell_color = 'white'\n",
    "#     canvas.itemconfig(grid_cells[row][col], fill=cell_color)\n",
    "    \n",
    "# def update_grid():\n",
    "#     for row in range(3):\n",
    "#         for col in range(3):\n",
    "#             cell_value = grid_value[row][col]\n",
    "#             if cell_value == 1:\n",
    "#                 cell_color = 'blue'\n",
    "#             else:\n",
    "#                 cell_color = 'white'\n",
    "#             canvas.itemconfig(grid_cells[row][col], fill=cell_color)\n",
    "\n",
    "# def clear_grid():\n",
    "#     global grid_value\n",
    "#     grid_value = copy.deepcopy(initial_grid)\n",
    "#     max_moves_counter.config(text=f\"Moves Left: {sum(solver_3(grid_value))}\")\n",
    "#     update_grid()\n",
    "#     root.update()\n",
    "    \n",
    "    \n",
    "# def highlight_cell(row, col):\n",
    "#     canvas.itemconfig(grid_cells[row][col], fill='yellow')\n",
    "#     root.update()\n",
    "#     time.sleep(0.5)  \n",
    "#     canvas.itemconfig(grid_cells[row][col], fill='white')\n",
    "#     root.update()\n",
    "    \n",
    "    \n",
    "# def bot_instruction():\n",
    "#     solution = solver_3(grid_value)\n",
    "#     sol_mat = [solution[i:i+3] for i in range(0, len(solution), 3)]\n",
    "#     for row in range(3):\n",
    "#         for col in range(3):\n",
    "#             if sol_mat[row][col] == 1:\n",
    "#                 highlight_cell(row, col)\n",
    "#                 on_grid_click(row,col)\n",
    "#                 root.update()\n",
    "#                 time.sleep(1)\n",
    "            \n",
    "    \n",
    "# def new_game():\n",
    "#     global initial_grid, grid_value\n",
    "#     initial_grid = []\n",
    "#     for _ in range(3):\n",
    "#         initial_grid.append([randint(0,1) for b in range(0,3)])\n",
    "#     grid_value = copy.deepcopy(initial_grid)\n",
    "#     update_grid()\n",
    "#     root.update()\n",
    "    \n",
    "    \n",
    "# # Create the main window\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Lights Out- 3 by 3\")\n",
    "\n",
    "# # Create a canvas to draw the grid\n",
    "# canvas = tk.Canvas(root, width=300, height=300)\n",
    "# canvas.pack()\n",
    "\n",
    "# # Initialize grid value\n",
    "# # 1 - Blue, 0 - White\n",
    "\n",
    "# # Grid Creation\n",
    "# cell_size = 100\n",
    "# grid_cells = []\n",
    "# for row in range(3):\n",
    "#     grid_cells.append([])\n",
    "#     for col in range(3):\n",
    "#         x0, y0 = col * cell_size, row * cell_size\n",
    "#         x1, y1 = x0 + cell_size, y0 + cell_size\n",
    "#         cell = canvas.create_rectangle(x0, y0, x1, y1, fill= 'white', outline=\"black\")\n",
    "#         grid_cells[row].append(cell)\n",
    "#         canvas.tag_bind(cell, \"<Button-1>\", lambda event, row=row, col=col: on_grid_click(row, col))\n",
    "#         canvas.tag_bind(cell, \"<Enter>\", lambda event, row=row, col=col: on_enter(event, row, col))\n",
    "#         canvas.tag_bind(cell, \"<Leave>\", lambda event, row=row, col=col: on_leave(event, row, col))\n",
    "        \n",
    "# clear_button = tk.Button(root, text=\"Restart\", command=clear_grid)\n",
    "# clear_button.pack()\n",
    "\n",
    "# bot_button = tk.Button(root, text=\"Auto solver\", command=bot_instruction)\n",
    "# bot_button.pack()\n",
    "\n",
    "# puzzle_button = tk.Button(root, text=\"New pattern\", command=new_game)\n",
    "# puzzle_button.pack()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #print_button = tk.Button(root, text=\"Print Grid State\", command=print_grid_state)\n",
    "# #print_button.pack()\n",
    "\n",
    "# # Start the Tkinter event loop\n",
    "# #new_game()\n",
    "# initial_grid = [[1,1,0],[0,1,1],[1,0,0]]\n",
    "# #for _ in range(3):\n",
    "#         #initial_grid.append([randint(0,1) for b in range(0,3)])\n",
    "# grid_value = copy.deepcopy(initial_grid)\n",
    "# update_grid()\n",
    "# max_moves_counter = tk.Label(root, text=f\"Moves Needed: {sum(solver_3(grid_value))}\")\n",
    "# max_moves_counter.pack()\n",
    "\n",
    "# root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1952cd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 0), (2, 2), (1, 1), (0, 0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_grid = [[0,0,1],[1,1,1],[1,0,0]]\n",
    "grid_value = copy.deepcopy(initial_grid)\n",
    "\n",
    "def solution(grid_value):\n",
    "    if is_terminal(grid_value):\n",
    "        return []\n",
    "    else:\n",
    "        state = (np.random.randint(3),np.random.randint(3))\n",
    "        solution = []\n",
    "        solution.append(state)\n",
    "        action_click(state)\n",
    "        while not is_terminal(grid_value):\n",
    "            grid = tuple([tuple(i) for i in grid_value])\n",
    "            next_act, val = best_action(grid, q_values)\n",
    "            action_click(next_act)\n",
    "            solution.append(next_act)\n",
    "        return solution\n",
    "\n",
    "solution(grid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2840ce23",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m                 states_dict[(\u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mtuple\u001b[39m(binary_array[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m5\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m5\u001b[39m)]), (x,y))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m states_dict\n\u001b[1;32m---> 12\u001b[0m q_values \u001b[38;5;241m=\u001b[39m states_with_actions(\u001b[38;5;241m25\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m, in \u001b[0;36mstates_with_actions\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m             states_dict[(\u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mtuple\u001b[39m(binary_array[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m5\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m5\u001b[39m)]), (x,y))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m states_dict\n",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m             states_dict[(\u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mtuple\u001b[39m(binary_array[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m5\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m5\u001b[39m)]), (x,y))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m states_dict\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def states_with_actions(n):\n",
    "    states_dict = {}\n",
    "    for i in range(2**n):\n",
    "        binary_array = []\n",
    "        for j in range(n):\n",
    "            binary_array.append((i >> j) & 1)\n",
    "        for x in range(5):\n",
    "            for y in range(5):\n",
    "                states_dict[(tuple([tuple(binary_array[i:i+5]) for i in range(0,25,5)]), (x,y))] = 0\n",
    "    return states_dict\n",
    "\n",
    "q_values = states_with_actions(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7f1224",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q_values\n",
      "\u001b[1;31mNameError\u001b[0m: name 'q_values' is not defined"
     ]
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71afb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_states_KO.pkl', 'wb') as f:\n",
    "    pickle.dump(q_values, f)\n",
    "        \n",
    "with open('all_states_KO.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
